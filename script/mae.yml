task: pretrain
is_train: true
architecture: mae
batch_size: 800
input_len: 120
pred_len: 60
d_model: 512
nhead: 8
e_layers: 6
d_layers: 6
dropout: 0.1
learning_rate: 0.0001
warmup_steps: 200
pos_emb: 'nrml'
