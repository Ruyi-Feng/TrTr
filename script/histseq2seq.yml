task: pretrain
is_train: true
architecture: histseq2seq
train_epochs: 100
batch_size: 1024
input_len: 120
pred_len: 60
shared_pos_embed: false
d_model: 512
nhead: 8
e_layers: 6
d_layers: 6
dropout: 0.1
learning_rate: 0.0001
warmup_steps: 800
model_type: 'nrml'
